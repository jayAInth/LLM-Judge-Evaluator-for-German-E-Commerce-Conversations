# =============================================================================
# LLM Judge Framework - Environment Configuration
# =============================================================================
# SECURITY: Do NOT commit real secrets to version control!
# For production, use a secrets manager (Vault, AWS Secrets Manager, etc.)
# =============================================================================

DEBUG=true

# --- Database Configuration ---
POSTGRES_USER=llmjudge
POSTGRES_PASSWORD=change_this_in_production
POSTGRES_DB=llmjudge

# --- HuggingFace Token (required for vLLM with gated models) ---
# Get your token from: https://huggingface.co/settings/tokens
HF_TOKEN=your_hf_token_here

# --- LLM Judge Configuration (Ollama - Default) ---
JUDGE_MODEL_PROVIDER=openai_compatible
JUDGE_MODEL_NAME=gpt-oss-120b
JUDGE_API_URL=http://host.docker.internal:11434/v1
JUDGE_API_KEY=ollama

# --- LLM Judge Configuration (vLLM - GPU Required) ---
# Uncomment to use the high-performance 72B model (requires docker-compose --profile gpu up)
# JUDGE_MODEL_PROVIDER=openai_compatible
# JUDGE_MODEL_NAME=Qwen/Qwen2.5-72B-Instruct-GPTQ-Int4
# JUDGE_API_URL=http://vllm:8000/v1
